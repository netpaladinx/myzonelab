import copy
import math
import warnings

import torch
import torch.nn as nn

from ..registry import INITIALIZERS
from ..utils import get_root_logger, load_model


class BaseInitializer:
    def __init__(self, module_type=None):
        self.module_type = module_type

    def require_init(self, m):
        if not self.module_type:
            return True

        name_stack = set([m.__class__.__name__] +
                         [b.__name__ for b in m.__class__.__bases__])
        if isinstance(self.module_type, str) and self.module_type in name_stack:
            return True
        elif len(set(self.module_type) & name_stack):
            return True

        return False


def constant_init(module, weight, bias=0):
    if hasattr(module, 'weight') and module.weight is not None:
        nn.init.constant_(module.weight, weight)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


@INITIALIZERS.register_class('constant')
class ConstantInitializer(BaseInitializer):
    def __init__(self, weight, bias=0, module_type=None):
        super().__init__(module_type)
        self.weight = weight
        self.bias = bias

    def __call__(self, module):
        def init(m):
            if self.require_init(m):
                constant_init(m, self.weight, self.bias)

        module.apply(init)


def xavier_init(module, gain=1, bias=0, distribution='normal'):
    assert distribution in ['normal', 'uniform']
    if hasattr(module, 'weight') and module.weight is not None:
        if distribution == 'normal':
            nn.init.xavier_normal_(module.weight, gain=gain)
        else:
            nn.init.xavier_uniform_(module.weight, gain=gain)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


@INITIALIZERS.register_class('xavier')
class XavierInitializer(BaseInitializer):
    def __init__(self, gain=1, bias=0, distribution='normal', module_type=None):
        super().__init__(module_type)
        self.gain = gain
        self.bias = bias
        self.distribution = distribution

    def __call__(self, module):
        def init(m):
            if self.require_init(m):
                xavier_init(m, self.gain, self.bias, self.distribution)

        module.apply(init)


def normal_init(module, mean=0, std=1, bias=0):
    if hasattr(module, 'weight') and module.weight is not None:
        nn.init.normal_(module.weight, mean, std)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


@INITIALIZERS.register_class('normal')
class NormalInitializer(BaseInitializer):
    def __init__(self, mean=0, std=1, bias=0, module_type=None):
        super().__init__(module_type)
        self.mean = mean
        self.std = std
        self.bias = bias

    def __call__(self, module):
        def init(m):
            if self.require_init(m):
                normal_init(m, self.mean, self.std, self.bias)

        module.apply(init)


def trunc_normal_(tensor, mean, std, low, high):
    def norm_cdf(x):  # standard normal cummulative distribution
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if mean < low - 2 * std or mean > high + 2 * std:
        warnings.warn('mean is more than 2 std from [low, high] in trunc_normal. '
                      'The distribution of values may be incorrect.',
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    lower = norm_cdf((low - mean) / std)
    upper = norm_cdf((high - mean) / std)

    # Uniformly fill tensor with values from [lower, upper],
    # then translate to [2lower-1, 2upper-1].
    tensor.uniform_(2 * lower - 1, 2 * upper - 1)

    # Use inverse cdf transform for normal distribution to get truncated standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=low, max=high)
    return tensor


def trunc_normal_init(module, mean=0, std=1, low=-2, high=2, bias=0):
    if hasattr(module, 'weight') and module.weight is not None:
        with torch.no_grad():
            trunc_normal_(module.weight, mean, std, low, high)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


@INITIALIZERS.register_class('trunc_normal')
class TruncNormalInitializer(BaseInitializer):
    def __init__(self, mean=0, std=1, low=-2, high=2, bias=0, module_type=None):
        super().__init__(module_type)
        self.mean = mean
        self.std = std
        self.low = low
        self.high = high
        self.bias = bias

    def __call__(self, module):
        def init(m):
            if self.require_init(m):
                trunc_normal_init(m, self.mean, self.std, self.low, self.high, self.bias)

        module.apply(init)


def uniform_init(module, low=0, high=1, bias=0):
    if hasattr(module, 'weight') and module.weight is not None:
        nn.init.uniform_(module.weight, low, high)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


@INITIALIZERS.register_class('uniform')
class UniformInitializer(BaseInitializer):
    def __init__(self, low=0, high=1, bias=0, module_type=None):
        super().__init__(module_type)
        self.low = low
        self.high = high
        self.bias = bias

    def __call__(self, module):
        def init(m):
            if self.require_init(m):
                uniform_init(m, self.low, self.high, self.bias)

        module.apply(init)


def kaiming_init(module, a=0, mode='fan_out', nonlinearity='relu', bias=0, distribution='normal'):
    assert distribution in ['normal', 'uniform']
    if hasattr(module, 'weight') and module.weight is not None:
        if distribution == 'normal':
            nn.init.kaiming_normal_(module.weight, a=a, mode=mode, nonlinearity=nonlinearity)
        else:
            nn.init.kaiming_uniform_(module.weight, a=a, mode=mode, nonlinearity=nonlinearity)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


@INITIALIZERS.register_class('kaiming')
class KaimingInitializer(BaseInitializer):
    def __init__(self, a=0, mode='fan_out', nonlinearity='relu', bias=0, distribution='normal', module_type=None):
        super().__init__(module_type)
        self.a = a
        self.mode = mode
        self.nonlinearity = nonlinearity
        self.bias = bias
        self.distribution = distribution

    def __call__(self, module):
        def init(m):
            if self.require_init(m):
                kaiming_init(m, self.a, self.mode, self.nonlinearity, self.bias, self.distribution)

        module.apply(init)


@INITIALIZERS.register_class('caffe2xavier')
class Caffe2XavierInitializer(KaimingInitializer):
    def __init__(self, a=1, mode='fan_in', nonlinearity='leaky_relu', bias=0, distribution='uniform', module_type=None):
        super().__init__(a=a,
                         mode=mode,
                         nonlinearity=nonlinearity,
                         bias=bias,
                         distribution=distribution,
                         module_type=module_type)


@INITIALIZERS.register_class('pretrained')
class PretrainedInitializer:
    def __init__(self, path, src_prefix=None, dst_prefix=None, map_location=None, strict=False):
        self.ckpt_path = path
        self.src_prefix = src_prefix
        self.dst_prefix = dst_prefix
        self.map_location = map_location
        self.strict = strict

    def __call__(self, module=None):
        logger = get_root_logger()
        logger.info(f"Load model from: {self.ckpt_path}")

        return load_model(self.ckpt_path,
                          map_location=self.map_location,
                          src_prefix=self.src_prefix,
                          dst_prefix=self.dst_prefix,
                          model=module,
                          strict=self.strict)


@INITIALIZERS.register_class('checkpoint')
class CheckpointLoader(PretrainedInitializer):
    def __init__(self, path, map_location=None):
        super().__init__(path, map_location=map_location, strict=True)


def initialize(module, init_cfg):
    assert isinstance(init_cfg, (dict, list, tuple))
    if isinstance(init_cfg, dict):
        init_cfg = [init_cfg]

    self_cfg, child_cfg = [], []
    for cfg in init_cfg:
        if 'child' in cfg:
            child_cfg.append(copy.deepcopy(cfg))
        else:
            self_cfg.append(copy.deepcopy(cfg))

    for cfg in self_cfg:
        initializer = INITIALIZERS.create(cfg)
        initializer(module)

    for cfg in child_cfg:
        child = cfg.pop('child')
        if hasattr(module, child):
            child = getattr(module, child)
            if isinstance(child, nn.Module):
                initializer = INITIALIZERS.create(cfg)
                initializer(child)


def build_module_from_init(init_cfg):
    assert isinstance(init_cfg, (dict, list, tuple))
    if isinstance(init_cfg, (list, tuple)):
        init_cfg = init_cfg[0]  # use the first init config to build

    initializer = INITIALIZERS.create(init_cfg)
    return initializer()
